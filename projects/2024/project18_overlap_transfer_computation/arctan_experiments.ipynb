{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overlapping data transfer and computations with arctan dummy computation\n",
    "\n",
    "*Disclaimer: we used the venv from the HPC4WC course, see https://github.com/ofuhrer/HPC4WC for the setup.\n",
    "\n",
    "This file includes experimental code used along the way of finding the 'optimal' solution. It is included more for completeness, the important results with explanations are located in the `Report.ipynb`. I have added some comments (marked as 'Comment: ' so my process and thoughts are more understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import cupy as cp\n",
    "import cupyx as cpx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: I was experimenting with numpy/cupy as I thought we might benchmark against numpy version aswell. We decided against that (obviously this would greatly distort results as numpy is way slower than cupy) because we wanted to get an efficiency gain using the same architecture & libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_np_iterate_arctan(x, i):\n",
    "    result = x\n",
    "    for _ in range(i):\n",
    "        result = np.arctan(result)\n",
    "    return result\n",
    "\n",
    "def naive_cp_iterate_arctan(x, i):\n",
    "    result = x\n",
    "    for _ in range(i):\n",
    "        result = cp.arctan(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the random array with numpy\n",
    "def generate_random_array_np(n):\n",
    "    return np.random.rand(n)\n",
    "\n",
    "# generate the random array with random\n",
    "def naive_generate_random_array(n):\n",
    "    return [random.random() for _ in range(n)]\n",
    "\n",
    "# generate the random array directly on the gpu\n",
    "def generate_random_array_cp(n):\n",
    "    return cp.random.random(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy naive implementation\n",
    "def np_timed(num_elems, iters):\n",
    "    random_array = generate_random_array_np(num_elems)\n",
    "\n",
    "    start_time = time.time()\n",
    "    result_array = naive_np_iterate_arctan(random_array, iters)\n",
    "    computation_duration = time.time() - start_time\n",
    "    print(\n",
    "        f\"number of elements: {num_elems}, iterations: {iters},  computation time np: {computation_duration:.6f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: this was before we realized that the copy to device cannot be done asynchronously with the older cupy version. Note that I'm freeing memory at the end because with my experiments I kept running out of RAM on the GPU as I used very large arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cupy with transfer\n",
    "def cp_timed(num_elems, iters):\n",
    "    random_array = generate_random_array_np(num_elems)\n",
    "    T_out = cpx.zeros_pinned(random_array.shape, dtype=np.float64)\n",
    "\n",
    "    # transfer\n",
    "    start_time = time.time()\n",
    "    random_gpu_array = cp.asarray(random_array)\n",
    "    transfer_duration = time.time() - start_time\n",
    "    print(f\"Transfer duration: {transfer_duration:.6f} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    random_gpu_array = naive_cp_iterate_arctan(random_gpu_array, iters)\n",
    "    computation_duration = time.time() - start_time\n",
    "    print(f\"Computation time CuPy: {computation_duration:.6f} seconds\")\n",
    "\n",
    "    # Copy result back to CPU\n",
    "    start_time = time.time()\n",
    "    random_gpu_array.get(out=T_out)\n",
    "    # result_array = cp.asnumpy(random_gpu_array)\n",
    "    cp.cuda.runtime.deviceSynchronize()\n",
    "    copyback_duration = time.time() - start_time\n",
    "    print(f\"Copy back to CPU duration: {copyback_duration:.6f} seconds\")\n",
    "\n",
    "    print(\n",
    "        f\"number of elements: {num_elems}, iterations: {iters}, total computation time naive cp: {transfer_duration + computation_duration + copyback_duration:.6f}\"\n",
    "    )\n",
    "\n",
    "    del random_array, random_gpu_array\n",
    "    cp.get_default_memory_pool().free_all_blocks() # ran into out of memory problems because I experimented with too large arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: This is a left-over of me not understanding streams, naively I tried using 3 streams, one for copying HtoD, one for the computations, and one for copying DtoH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelining\n",
    "def cp_pipelining(num_elems, iters, block_size):\n",
    "    random_array = generate_random_array_np(num_elems)\n",
    "\n",
    "    # Transfer and compute in blocks\n",
    "    start_time = time.time()\n",
    "    streams = [\n",
    "        cp.cuda.Stream(non_blocking=True) for _ in range(3)\n",
    "    ]  # Create two streams for overlap\n",
    "\n",
    "    result_blocks = []\n",
    "    for i in range(0, n, block_size):\n",
    "        block = random_array[i : i + block_size]\n",
    "        with streams[0]:  # Transfer to GPU\n",
    "            gpu_block = cp.asarray(block)\n",
    "\n",
    "        with streams[1]:  # Compute on GPU\n",
    "            result_block = naive_cp_iterate_arctan(gpu_block, iters)\n",
    "            result_blocks.append(result_block)\n",
    "\n",
    "        with streams[2]:  # Copy back to CPU\n",
    "            result_cpu_block = cp.asnumpy(result_block)\n",
    "            result_blocks.append(result_cpu_block)\n",
    "\n",
    "        del gpu_block, result_block\n",
    "    # Ensure all operations are completed\n",
    "    for stream in streams:\n",
    "        stream.synchronize()\n",
    "\n",
    "    transfer_computation_duration = time.time() - start_time\n",
    "    print(\n",
    "        f\"number of elements: {num_elems}, iterations: {iters}, block_size: {block_size}, total computation time pipelining cp: {transfer_computation_duration:.6f}\"\n",
    "    )\n",
    "    cp.get_default_memory_pool().free_all_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: Same as before. But maybe pinned memory fixes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinned memory\n",
    "def cp_pipelining_pinned_mem(num_elems, iters, block_size):\n",
    "    random_array = generate_random_array_np(num_elems)\n",
    "\n",
    "    # Allocate pinned memory\n",
    "    pinned_memory = cp.cuda.alloc_pinned_memory(random_array.nbytes)\n",
    "    pinned_array = np.frombuffer(pinned_memory, random_array.dtype).reshape(\n",
    "        random_array.shape\n",
    "    )\n",
    "    np.copyto(pinned_array, random_array)\n",
    "\n",
    "    # Transfer and compute in blocks\n",
    "    start_time = time.time()\n",
    "    streams = [cp.cuda.Stream(non_blocking=True) for _ in range(3)]\n",
    "\n",
    "    result_blocks = []\n",
    "    for i in range(0, num_elems, block_size):\n",
    "        block = pinned_array[i : i + block_size]\n",
    "\n",
    "        with streams[0]:  # Transfer to GPU\n",
    "            gpu_block = cp.asarray(block)\n",
    "\n",
    "        with streams[1]:  # Compute on GPU\n",
    "            result_block = naive_cp_iterate_arctan(gpu_block, iters)\n",
    "\n",
    "        with streams[2]:  # Copy back to CPU\n",
    "            result_cpu_block = cp.asnumpy(result_block)\n",
    "            result_blocks.append(result_cpu_block)\n",
    "\n",
    "        # Free GPU memory\n",
    "        del gpu_block, result_block\n",
    "\n",
    "    # Ensure all operations are completed\n",
    "    for stream in streams:\n",
    "        stream.synchronize()\n",
    "\n",
    "    transfer_computation_duration = time.time() - start_time\n",
    "    print(\n",
    "        f\"number of elements: {num_elems}, iterations: {iters}, block_size: {block_size}, total computation time pipelining cp: {transfer_computation_duration:.6f}\"\n",
    "    )\n",
    "    del result_blocks\n",
    "    cp.get_default_memory_pool().free_all_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: We've understood that streams work differently, and even better with pinned memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelining\n",
    "def cp_pipelining_all(num_elems, iters, block_size):\n",
    "    random_array = generate_random_array_np(num_elems)\n",
    "    T_out = cpx.zeros_pinned(random_array.shape, dtype=np.float64)\n",
    "\n",
    "    # Transfer and compute in blocks\n",
    "    start_time = time.time()\n",
    "    streams = [\n",
    "        cp.cuda.Stream(non_blocking=True) for _ in range(0, num_elems, block_size)\n",
    "    ]  # Create streams for overlap\n",
    "\n",
    "    result_blocks = []\n",
    "    for i, s in zip(range(0, num_elems, block_size), streams):\n",
    "        block = random_array[i : i + block_size]\n",
    "\n",
    "        with s:\n",
    "            gpu_block = cp.asarray(block)\n",
    "            result_block = naive_cp_iterate_arctan(gpu_block, iters)\n",
    "            result_blocks.append(result_block)\n",
    "            result_block.get(out=T_out[i : i + block_size])\n",
    "        del gpu_block, result_block\n",
    "\n",
    "    # Ensure all operations are completed\n",
    "    for stream in streams:\n",
    "        stream.synchronize()\n",
    "\n",
    "    transfer_computation_duration = time.time() - start_time\n",
    "    print(\n",
    "        f\"number of elements: {num_elems}, iterations: {iters}, block_size: {block_size}, total computation time pipelining cp: {transfer_computation_duration:.6f}\"\n",
    "    )\n",
    "    cp.get_default_memory_pool().free_all_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2**20\n",
    "iterations = 10**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements: 1048576, iterations: 1000,  computation time np: 18.018865\n"
     ]
    }
   ],
   "source": [
    "np_timed(n, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer duration: 0.001263 seconds\n",
      "Computation time CuPy: 0.011194 seconds\n",
      "Copy back to CPU duration: 0.024143 seconds\n",
      "number of elements: 1048576, iterations: 1000, total computation time naive cp: 0.036600\n"
     ]
    }
   ],
   "source": [
    "cp_timed(n, iterations) #gpu needs some warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer duration: 0.000970 seconds\n",
      "Computation time CuPy: 0.010591 seconds\n",
      "Copy back to CPU duration: 0.024732 seconds\n",
      "number of elements: 1048576, iterations: 1000, total computation time naive cp: 0.036292\n"
     ]
    }
   ],
   "source": [
    "cp_timed(n, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: I wonder why the 'pipelining' is not faster than the normal synchronous call :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements: 1048576, iterations: 1000, block_size: 1048576, total computation time pipelining cp: 0.035711\n"
     ]
    }
   ],
   "source": [
    "cp_pipelining(n, iterations, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2**24\n",
    "iterations = 2 * 10**3\n",
    "\n",
    "elements_sizes_array = np.array([2**i for i in range(14, 24)])\n",
    "iterations_array = np.array([2**i for i in range(8, 12)])\n",
    "block_sizes_array = np.array([int(n / 2**i) for i in range(3, 7)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: Wanted to find a configuration where we have speedups, but that didn't go that well ended up using only different block sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer duration: 0.050344 seconds\n",
      "Computation time CuPy: 0.498564 seconds\n",
      "Copy back to CPU duration: 0.520006 seconds\n",
      "number of elements: 16777216, iterations: 2000, total computation time naive cp: 1.068914\n"
     ]
    }
   ],
   "source": [
    "cp_timed(n, iterations)  # for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements: 16777216, iterations: 2000, block_size: 2097152, total computation time pipelining cp: 1.005827\n",
      "number of elements: 16777216, iterations: 2000, block_size: 1048576, total computation time pipelining cp: 1.005080\n",
      "number of elements: 16777216, iterations: 2000, block_size: 524288, total computation time pipelining cp: 1.006255\n",
      "number of elements: 16777216, iterations: 2000, block_size: 262144, total computation time pipelining cp: 1.274939\n"
     ]
    }
   ],
   "source": [
    "for block_size in block_sizes_array:\n",
    "    cp_pipelining_all(n, iterations, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements: 16777216, iterations: 2000, block_size: 2097152, total computation time pipelining cp: 1.042706\n",
      "number of elements: 16777216, iterations: 2000, block_size: 1048576, total computation time pipelining cp: 1.081355\n",
      "number of elements: 16777216, iterations: 2000, block_size: 524288, total computation time pipelining cp: 1.142774\n",
      "number of elements: 16777216, iterations: 2000, block_size: 262144, total computation time pipelining cp: 1.333033\n"
     ]
    }
   ],
   "source": [
    "for block_size in block_sizes_array:\n",
    "    cp_pipelining(n, iterations, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements: 16777216, iterations: 2000, block_size: 2097152, total computation time pipelining cp: 1.043696\n",
      "number of elements: 16777216, iterations: 2000, block_size: 1048576, total computation time pipelining cp: 1.081404\n",
      "number of elements: 16777216, iterations: 2000, block_size: 524288, total computation time pipelining cp: 1.144172\n",
      "number of elements: 16777216, iterations: 2000, block_size: 262144, total computation time pipelining cp: 1.300752\n"
     ]
    }
   ],
   "source": [
    "for block_size in block_sizes_array:\n",
    "    cp_pipelining_pinned_mem(n, iterations, block_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: cp_pipelining_all should've been faster, but it wasn't. The story continues in the main file..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPC4WC_kernel",
   "language": "python",
   "name": "hpc4wc_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
